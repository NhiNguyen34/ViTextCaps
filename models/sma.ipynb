{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft 1: Run SMA in MMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --root-user-action=ignore requests\n",
    "!pip install --root-user-action=ignore\n",
    "!pip install -q transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download data\n",
    "\n",
    "!pip install kaggle\n",
    "\n",
    "!sudo rm -rf /root/.kaggle\n",
    "\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "\n",
    "%cd .\n",
    "\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "\n",
    "!sudo chown `whoami` ~/.kaggle/kaggle.json\n",
    "\n",
    "\n",
    "\n",
    "!export KAGGLE_CONFIG_DIR='/.kaggle/'\n",
    "\n",
    "%cd .\n",
    "\n",
    "!kaggle datasets download -d anhnguyen14/vitextcap-combined-data\n",
    "\n",
    "!ls '/workspace'\n",
    "\n",
    "%cd /workspace/M4C\n",
    "!ls\n",
    "\n",
    "\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge_score\n",
    "!pip install --upgrade nltk\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "# !unzip -q '/content/drive/MyDrive/ViTextCap/Data/Object Features/vinvl_extraction.zip' -d Object_Features\n",
    "# !unzip -q /content/drive/MyDrive/ViTextCap/Data/FastText.zip -d FastText\n",
    "# !unzip -q '/content/drive/MyDrive/ViTextCap/Data/SwimTextSpotter/ocr_1.zip.zip' -d ocr_features\n",
    "# !unzip -q '/content/drive/MyDrive/ViTextCap/Data/SwimTextSpotter/ocr_2.zip.zip' -d ocr_features\n",
    "\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install unzip\n",
    "\n",
    "\n",
    "%cd .\n",
    "\n",
    "\n",
    "!ls\n",
    "\n",
    "!unzip -q '/content/drive/MyDrive/ViTextCap/Data/combined_data.zip' -d combined_data\n",
    "\n",
    "\n",
    "## .\n",
    "\n",
    "!pip install pycocoevalcap\n",
    "\n",
    "\n",
    "!pip -q install evaluate\n",
    "\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "import nltk\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('punkt')\n",
    "\n",
    "class ViTextCapsDataset(Dataset):\n",
    "  def __init__(self, tokenizer, data_folder_path=None, mask_value=64000.0):\n",
    "\n",
    "    self.data_paths=glob(data_folder_path+'/*')\n",
    "    self.dummy_tensor = torch.ones((1, 300))\n",
    "    self.mask_value = mask_value\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = np.load(self.data_paths[idx], allow_pickle=True).item()\n",
    "    return {\n",
    "            'id': sample['image_id'],\n",
    "            'captions': sample['captions'],\n",
    "            'obj_boxes': torch.tensor(sample['obj']['boxes']),\n",
    "            'obj_features': torch.tensor(sample['obj']['features']),\n",
    "            'ocr_texts': sample['ocr']['texts'],\n",
    "            'ocr_boxes': torch.tensor(sample['ocr']['boxes']),\n",
    "            'ocr_token_embeddings': torch.tensor(sample['ocr']['fasttext_token']) if len(sample['ocr']['fasttext_token']) > 0 else self.dummy_tensor,\n",
    "            'ocr_rec_features': torch.tensor(sample['ocr']['rec_features']),\n",
    "            'ocr_det_features': torch.tensor(sample['ocr']['det_features'])\n",
    "        }\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_paths)\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "\n",
    "    raw_captions = []\n",
    "    captions_ = []\n",
    "    obj_boxes_tensor = []\n",
    "    obj_features_tensor = []\n",
    "    ocr_boxes_tensor = []\n",
    "    ocr_token_embeddings_tensor = []\n",
    "    ocr_rec_features_tensor = []\n",
    "    ocr_det_features_tensor = []\n",
    "    texts_ = []\n",
    "\n",
    "\n",
    "    for each in batch:\n",
    "      captions = each['captions']\n",
    "      for i in range(len(each['captions'])):\n",
    "        raw_captions.append(captions)\n",
    "        #print('raw_1',raw_captions)\n",
    "        captions_.append(each['captions'][i])\n",
    "        #print('captions_1',captions_)\n",
    "\n",
    "      obj_boxes_tensor.extend([each['obj_boxes']]*len(captions))\n",
    "      obj_features_tensor.extend([each['obj_features']]*len(captions))\n",
    "\n",
    "      ocr_boxes_tensor.extend([each['ocr_boxes']]*len(captions))\n",
    "      ocr_token_embeddings_tensor.extend([each['ocr_token_embeddings']]*len(captions))\n",
    "      ocr_rec_features_tensor.extend([each['ocr_rec_features']]*len(captions))\n",
    "      ocr_det_features_tensor.extend([each['ocr_det_features']]*len(captions))\n",
    "\n",
    "      texts_.extend([each['ocr_texts']]*len(captions))\n",
    "\n",
    "    # Convert obj list to tensor\n",
    "    #print(torch.tensor(obj_boxes_tensor).shape)\n",
    "    obj_boxes_tensor = torch.stack(obj_boxes_tensor)\n",
    "    #print(torch.tensor(obj_features_tensor).shape)\n",
    "    obj_features_tensor = torch.stack(obj_features_tensor)\n",
    "\n",
    "    #print('****\\n',obj_boxes_tensor.shape)\n",
    "    #print(obj_features_tensor.shape)\n",
    "\n",
    "    #print(obj_boxes_tensor.shape)\n",
    "    #print(obj_features_tensor.shape)\n",
    "    # Convert ocr list to tensor\n",
    "    ocr_boxes_tensor = pad_sequence(ocr_boxes_tensor, batch_first=True, padding_value=self.mask_value)\n",
    "    ocr_token_embeddings_tensor = pad_sequence(ocr_token_embeddings_tensor, batch_first=True, padding_value=1)\n",
    "    ocr_rec_features_tensor = pad_sequence(ocr_rec_features_tensor, batch_first=True, padding_value=1)\n",
    "    ocr_det_features_tensor = pad_sequence(ocr_det_features_tensor, batch_first=True, padding_value=1)\n",
    "\n",
    "\n",
    "\n",
    "    vs = self.tokenizer.vocab_size + 1\n",
    "    labels_= []\n",
    "\n",
    "    # Captions to token\n",
    "    for i, caption in enumerate(captions_):\n",
    "      label_ = []\n",
    "\n",
    "      for token in word_tokenize(caption):\n",
    "\n",
    "          if token in texts_[i] and token not in self.tokenizer.get_vocab():\n",
    "            label_.append(texts_[i].index(token) + vs)\n",
    "          else:\n",
    "            label_ += self.tokenizer(token)['input_ids'][1: -1]\n",
    "\n",
    "      label_.append(2) # 2 is <eos> in tokenizer\n",
    "      labels_.append(torch.tensor(label_))\n",
    "\n",
    "    # Convert labels_ 2 tensor\n",
    "    labels_ = pad_sequence(labels_, batch_first=True, padding_value=1)\n",
    "\n",
    "    dec_mask = torch.ones_like(labels_)\n",
    "    dec_mask = dec_mask.masked_fill(labels_ == 1, 0) # batch_size, seq_length\n",
    "\n",
    "    # Get the ocr_attention_mask\n",
    "    ocr_attn_mask = torch.ones_like(ocr_boxes_tensor)\n",
    "    ocr_attn_mask = ocr_attn_mask.masked_fill(ocr_boxes_tensor == self.mask_value, 0)[:, :, 0] # batch_size, seq_length\n",
    "    ocr_boxes_tensor = ocr_boxes_tensor.masked_fill(ocr_boxes_tensor == self.mask_value, 1)\n",
    "\n",
    "    # Join attention_mask\n",
    "    obj_attn_mask = torch.ones(size=(obj_boxes_tensor.size(0), obj_boxes_tensor.size(1))) # batch_size, seq_length\n",
    "    join_attn_mask = torch.cat([obj_attn_mask, ocr_attn_mask, dec_mask], dim=-1)\n",
    "\n",
    "    return {\n",
    "          'obj_boxes': obj_boxes_tensor,\n",
    "          'obj_features': obj_features_tensor,\n",
    "          'ocr_boxes': ocr_boxes_tensor,\n",
    "          'ocr_token_embeddings': ocr_token_embeddings_tensor,\n",
    "          'ocr_rec_features': ocr_rec_features_tensor,\n",
    "          'ocr_det_features': ocr_det_features_tensor,\n",
    "          'join_attn_mask': join_attn_mask,\n",
    "          'labels': labels_,\n",
    "          'texts': texts_,\n",
    "          'raw_captions': raw_captions\n",
    "    }\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "#phobert_model = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "!pip -q install rouge_score\n",
    "\n",
    "# Create Dataset\n",
    "train = ViTextCapsDataset(tokenizer,\n",
    "                        'combined_data/train')\n",
    "dev = ViTextCapsDataset(tokenizer,\n",
    "                        'combined_data/dev')\n",
    "test = ViTextCapsDataset(tokenizer,\n",
    "                        'combined_data/test')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train, batch_size=4, shuffle=True, collate_fn=train.collate_fn)\n",
    "test_dataloader = DataLoader(test, batch_size=4, shuffle=True, collate_fn=test.collate_fn)\n",
    "dev_dataloader = DataLoader(dev, batch_size=4, shuffle=True, collate_fn=dev.collate_fn)\n",
    "\n",
    "next(iter(train_dataloader))\n",
    "\n",
    "import evaluate\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "meteor = evaluate.load('meteor')\n",
    "cider_scorer = Cider()\n",
    "\n",
    "\n",
    "\n",
    "class SMA(Pythia):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def build(self):\n",
    "        self.mmt_config = BertConfig(**self.config.mmt)\n",
    "        self.mmt = MMT(self.mmt_config)\n",
    "        self.so_to_mmt_in = nn.Linear(3*1536, self.mmt_config.hidden_size)\n",
    "        self.st_to_mmt_in = nn.Linear(3*1536, self.mmt_config.hidden_size)\n",
    "        self.so_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n",
    "        self.st_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n",
    "        self.so_drop = nn.Dropout(0.1)\n",
    "        self.st_drop = nn.Dropout(0.1)\n",
    "        self.linear_go_to_mmt_in = nn.Linear(2048, self.mmt_config.hidden_size)\n",
    "        self.linear_gt_to_mmt_in = nn.Linear(300, self.mmt_config.hidden_size)\n",
    "        self.go_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n",
    "        self.gt_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n",
    "        self.go_drop = nn.Dropout(0.1)\n",
    "        self.gt_drop = nn.Dropout(0.1)\n",
    "        self.linear_updated_ocr_to_mmt_in = nn.Linear(300, self.mmt_config.hidden_size)\n",
    "        self.updated_ocr_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n",
    "        self.updated_ocr_drop = nn.Dropout(self.config.ocr.dropout_prob)\n",
    "        self.linear_joint = nn.Linear(1536,768)\n",
    "        self.answer_processor =  registry.get(self._datasets[0] + \"_answer_processor\")\n",
    "        self.ocr_ptr_net = OcrPtrNet(**self.config.classifier.ocr_ptr_net)\n",
    "        # modules requiring custom learning rates (usually for finetuning)\n",
    "        self.finetune_modules = []\n",
    "        self._build_obj_encoding()\n",
    "        self._build_ocr_encoding()\n",
    "        # init feature embedding for \"image\"\n",
    "        setattr(self, \"image_feature_dim\", self.config[\"image_feature_dim\"])\n",
    "        self.feature_embeddings_out_dim = 0\n",
    "        feature_attn_model_params = self.config[\"image_feature_embeddings\"][0]\n",
    "        feature_embedding = ImageEmbedding(\n",
    "            getattr(self, \"image_feature_dim\"),\n",
    "            self.text_embeddings_out_dim,\n",
    "            **feature_attn_model_params\n",
    "        )\n",
    "        self.feature_embeddings_out_dim += feature_embedding.out_dim\n",
    "        self.feature_embeddings_out_dim *= getattr(self, \"image_feature_dim\")\n",
    "        setattr(\n",
    "            self, \"image_feature_embeddings_out_dim\", self.feature_embeddings_out_dim\n",
    "        )\n",
    "        del self.feature_embeddings_out_dim\n",
    "        setattr(\n",
    "            self,\n",
    "            \"image_feature_embedding\",\n",
    "            feature_embedding\n",
    "        )\n",
    "        # init feature embedding for \"context\"\n",
    "        setattr(self, \"context_feature_dim\", self.config[\"context_feature_dim\"])\n",
    "        self.feature_embeddings_out_dim = 0\n",
    "        feature_attn_model_params = self.config[\"context_feature_embeddings\"][0]\n",
    "        feature_embedding = ImageEmbedding(\n",
    "            getattr(self, \"context_feature_dim\"),\n",
    "            self.text_embeddings_out_dim,\n",
    "            **feature_attn_model_params\n",
    "        )\n",
    "        self.feature_embeddings_out_dim += feature_embedding.out_dim\n",
    "        self.feature_embeddings_out_dim *= getattr(self, \"context_feature_dim\")\n",
    "        setattr(\n",
    "            self, \"context_feature_embeddings_out_dim\", self.feature_embeddings_out_dim\n",
    "        )\n",
    "        del self.feature_embeddings_out_dim\n",
    "        setattr(\n",
    "            self,\n",
    "            \"context_feature_embedding\",\n",
    "            feature_embedding\n",
    "        )\n",
    "        num_choices = registry.get(self._datasets[0] + \"_num_final_outputs\")\n",
    "        self.classifier = ClassifierLayer(\n",
    "            self.config[\"classifier\"][\"type\"],\n",
    "            in_dim=768,\n",
    "            out_dim=num_choices-50,\n",
    "            **self.config[\"classifier\"][\"params\"]\n",
    "        )\n",
    "        # Modify the output layer to output a sequence of words\n",
    "        self.classifier = nn.LSTM(input_size=self.mmt_config.hidden_size, hidden_size=self.mmt_config.hidden_size, num_layers=1)\n",
    "        # Add a linear layer to map the LSTM output to the vocabulary size\n",
    "        self.linear = nn.Linear(self.mmt_config.hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def _build_obj_encoding(self):\n",
    "        self.obj_dim = 2048\n",
    "        # object appearance feature: Faster R-CNN\n",
    "        self.obj_faster_rcnn_fc7 = ImageEncoder(\n",
    "            encoder_type='finetune_faster_rcnn_fpn_fc7',\n",
    "            in_dim=2048,\n",
    "            weights_file='detectron/fc6/fc7_w.pkl',\n",
    "            bias_file='detectron/fc6/fc7_b.pkl',\n",
    "            model_data_dir=self.config[\"model_data_dir\"]\n",
    "        )\n",
    "        # apply smaller lr to pretrained Faster R-CNN fc7\n",
    "        self.finetune_modules.append({\n",
    "            'module': self.obj_faster_rcnn_fc7,\n",
    "            'lr_scale': 0.1,\n",
    "        })\n",
    "        # OBJ location feature: relative bounding box coordinates (4-dim)\n",
    "        self.linear_obj_bbox_to_mmt_in = nn.Linear(\n",
    "            4, self.obj_dim\n",
    "        )\n",
    "        self.obj_feat_layer_norm = nn.LayerNorm(self.obj_dim)\n",
    "        self.obj_bbox_layer_norm = nn.LayerNorm(self.obj_dim)\n",
    "        self.obj_drop = nn.Dropout(0.1)\n",
    "\n",
    "    def _build_ocr_encoding(self):\n",
    "        self.ocr_fastext_dim = 300\n",
    "        self.ocr_phoc_dim = 604\n",
    "        self.ocr_RCNN_dim = 2048\n",
    "        self.transformer_cnn_dim = 512\n",
    "        # OCR appearance feature: Faster R-CNN\n",
    "        self.ocr_faster_rcnn_fc7 = ImageEncoder(\n",
    "            encoder_type='finetune_faster_rcnn_fpn_fc7',\n",
    "            in_dim=2048,\n",
    "            weights_file='detectron/fc6/fc7_w.pkl',\n",
    "            bias_file='detectron/fc6/fc7_b.pkl',\n",
    "            model_data_dir=self.config[\"model_data_dir\"]\n",
    "        )\n",
    "        self.finetune_modules.append({\n",
    "            'module': self.ocr_faster_rcnn_fc7,\n",
    "            'lr_scale': 0.1,\n",
    "        })\n",
    "\n",
    "        # OCR appearance feature: relative Fasttext + PHOC + FasterRCNN\n",
    "        self.linear_ocr_appear_to_mmt_in = nn.Linear(\n",
    "            self.ocr_fastext_dim+self.ocr_RCNN_dim+self.ocr_phoc_dim+self.transformer_cnn_dim, self.ocr_fastext_dim\n",
    "            # self.ocr_fastext_dim+self.ocr_RCNN_dim+self.ocr_phoc_dim, self.ocr_fastext_dim\n",
    "        )\n",
    "        # OCR location feature: relative bounding box coordinates (4-dim)\n",
    "        self.linear_ocr_bbox_to_mmt_in = nn.Linear(\n",
    "            4, self.ocr_fastext_dim\n",
    "        )\n",
    "        self.ocr_feat_layer_norm = nn.LayerNorm(self.ocr_fastext_dim)\n",
    "        self.ocr_bbox_layer_norm = nn.LayerNorm(self.ocr_fastext_dim)\n",
    "        self.ocr_drop = nn.Dropout(0.1)\n",
    "\n",
    "    def _forward_obj_encoding(self, sample_list):\n",
    "        # object appearance feature: Faster R-CNN fc7\n",
    "        obj_fc6 = sample_list.image_feature_0[:,:36,:]\n",
    "        obj_fc7 = self.obj_faster_rcnn_fc7(obj_fc6)\n",
    "        obj_fc7 = F.normalize(obj_fc7, dim=-1)\n",
    "\n",
    "        obj_feat = obj_fc7\n",
    "        obj_bbox = sample_list.obj_bbox[:,:36]\n",
    "        obj_mmt_in = (\n",
    "            self.obj_feat_layer_norm(\n",
    "                obj_feat\n",
    "            ) + self.obj_bbox_layer_norm(\n",
    "                self.linear_obj_bbox_to_mmt_in(obj_bbox)\n",
    "            )\n",
    "        )\n",
    "        obj_mmt_in = self.obj_drop(obj_mmt_in)\n",
    "        return obj_mmt_in\n",
    "\n",
    "    def _forward_ocr_encoding(self, sample_list):\n",
    "        # OCR FastText feature (300-dim)\n",
    "        ocr_fasttext = sample_list.context_feature_0\n",
    "        ocr_fasttext = F.normalize(ocr_fasttext, dim=-1)\n",
    "        assert ocr_fasttext.size(-1) == 300\n",
    "\n",
    "        # OCR PHOC feature (604-dim)\n",
    "        ocr_phoc = sample_list.context_phoc\n",
    "        ocr_phoc = F.normalize(ocr_phoc, dim=-1)\n",
    "        assert ocr_phoc.size(-1) == 604\n",
    "\n",
    "        # OCR appearance feature: Faster R-CNN fc7\n",
    "        ocr_fc6 = sample_list.image_feature_1[:,:ocr_fasttext.size(1),:]\n",
    "        ocr_fc7 = self.ocr_faster_rcnn_fc7(ocr_fc6)\n",
    "        ocr_fc7 = F.normalize(ocr_fc7, dim=-1)\n",
    "        assert ocr_fc7.size(-1) == 2048\n",
    "\n",
    "        # OCR appearance feature: Transformer global representation feature\n",
    "        ocr_trans = sample_list.image_feature_2[:,:ocr_fasttext.size(1),:]\n",
    "        ocr_trans = F.normalize(ocr_trans, dim=-1)\n",
    "        assert ocr_trans.size(-1) == 512\n",
    "\n",
    "        ocr_feat = torch.cat(\n",
    "            [ocr_fasttext, ocr_fc7, ocr_phoc, ocr_trans],\n",
    "            # [ocr_fasttext, ocr_fc7, ocr_phoc],\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        ocr_bbox = sample_list.ocr_bbox.coordinates\n",
    "        ocr_mmt_in = (\n",
    "                    self.ocr_feat_layer_norm(\n",
    "                        self.linear_ocr_appear_to_mmt_in(ocr_feat)\n",
    "                    ) + self.ocr_bbox_layer_norm(\n",
    "                        self.linear_ocr_bbox_to_mmt_in(ocr_bbox)\n",
    "                    )\n",
    "        )\n",
    "        ocr_mmt_in = self.ocr_drop(ocr_mmt_in)\n",
    "        return ocr_mmt_in\n",
    "\n",
    "    def get_optimizer_parameters(self, config):\n",
    "        optimizer_param_groups = []\n",
    "\n",
    "        base_lr = config.optimizer_attributes.params.lr\n",
    "        # collect all the parameters that need different/scaled lr\n",
    "        finetune_params_set = set()\n",
    "        for m in self.finetune_modules:\n",
    "            optimizer_param_groups.append({\n",
    "                \"params\": list(m['module'].parameters()),\n",
    "                \"lr\": base_lr * m['lr_scale']\n",
    "            })\n",
    "            finetune_params_set.update(list(m['module'].parameters()))\n",
    "        # remaining_params are those parameters w/ default lr\n",
    "        remaining_params = [\n",
    "            p for p in self.parameters() if p not in finetune_params_set\n",
    "        ]\n",
    "        # put the default lr parameters at the beginning\n",
    "        # so that the printed lr (of group 0) matches the default lr\n",
    "        optimizer_param_groups.insert(0, {\"params\": remaining_params})\n",
    "\n",
    "        return optimizer_param_groups\n",
    "\n",
    "    def forward(self, sample_list):\n",
    "        txt_inds = sample_list.text\n",
    "        txt_mask = _get_mask(sample_list.text_len, sample_list.text.size(1))\n",
    "        text_bert_out = self.text_bert(txt_inds=txt_inds,txt_mask=txt_mask)\n",
    "        sample_list.text = text_bert_out\n",
    "\n",
    "        _, s_o, s_oo, s_ot, s_t, s_tt, s_to = self.process_text_embedding(sample_list)\n",
    "\n",
    "        obj_encoded_feats = self._forward_obj_encoding(sample_list)\n",
    "        ocr_encoded_feats = self._forward_ocr_encoding(sample_list)\n",
    "        g_o = self.process_feature_embedding(\n",
    "            \"image\", sample_list, s_o, s_homo=s_oo, s_hetero=s_ot, \n",
    "            pre_ques_embed=sample_list.text, obj_feats=obj_encoded_feats, ocr_feats=ocr_encoded_feats\n",
    "        )\n",
    "        g_t, updated_ocr = self.process_feature_embedding(\n",
    "            \"context\", sample_list, s_t, s_homo=s_tt, s_hetero=s_to, \n",
    "            pre_ques_embed=sample_list.text, obj_feats=obj_encoded_feats, ocr_feats=ocr_encoded_feats\n",
    "        )  # torch.Size([128, 350])\n",
    "\n",
    "        s_o = torch.cat((s_o,s_oo,s_ot),dim=-1)\n",
    "        s_t = torch.cat((s_t,s_tt,s_to),dim=-1)\n",
    "        s_o = self.so_drop(self.so_layer_norm(self.so_to_mmt_in(s_o.unsqueeze(1)) ))\n",
    "        s_t = self.st_drop(self.st_layer_norm(self.st_to_mmt_in(s_t.unsqueeze(1)) ))\n",
    "        so_mask = torch.ones(s_o.size(0),s_o.size(1),dtype=torch.float32,device=s_o.device)\n",
    "        st_mask = torch.ones(s_t.size(0),s_t.size(1),dtype=torch.float32,device=s_t.device)        \n",
    "        g_o = self.go_drop(self.go_layer_norm(self.linear_go_to_mmt_in(g_o)))\n",
    "        g_t = self.gt_drop(self.gt_layer_norm(self.linear_gt_to_mmt_in(g_t)))\n",
    "        go_mask = torch.ones(g_o.size(0),g_o.size(1),dtype=torch.float32,device=g_o.device)\n",
    "        gt_mask = torch.ones(g_t.size(0),g_t.size(1),dtype=torch.float32,device=g_t.device)\n",
    "\n",
    "        ocr_emb = self.updated_ocr_drop(self.updated_ocr_layer_norm(self.linear_updated_ocr_to_mmt_in(updated_ocr)))\n",
    "        ocr_tokens = sample_list.context\n",
    "        # binary mask of valid OCR vs padding\n",
    "        ocr_nums = sample_list.context_info_0.max_features\n",
    "        ocr_mask = _get_mask(ocr_nums, ocr_tokens.size(1))\n",
    "       \n",
    "        if self.training:\n",
    "            prev_inds = sample_list.train_prev_inds.clone()\n",
    "            mmt_results = self.mmt(\n",
    "                s_o, so_mask, s_t, st_mask, \n",
    "                g_o, go_mask, g_t, gt_mask, \n",
    "                ocr_emb=ocr_emb,\n",
    "                ocr_mask=ocr_mask,\n",
    "                fixed_ans_emb=self.classifier.module.weight,\n",
    "                prev_inds=prev_inds,\n",
    "            )\n",
    "            g_O = mmt_results[\"mmt_so_output\"]*mmt_results[\"mmt_go_output\"]\n",
    "            g_T = mmt_results[\"mmt_st_output\"]*mmt_results[\"mmt_gt_output\"]\n",
    "            update_joint_embedding = torch.cat((g_O, g_T),dim=-1) # torch.Size([32, 1, 1536])\n",
    "            update_joint_embedding = self.linear_joint(update_joint_embedding)\n",
    "            mmt_dec_output = mmt_results[\"mmt_dec_output\"] # torch.Size([32, 12, 768])\n",
    "            score_feature = torch.cat([update_joint_embedding, mmt_dec_output[:,1:,:]], dim=-2)\n",
    "            mmt_ocr_output = mmt_results[\"mmt_ocr_output\"] \n",
    "            fixed_scores = self.classifier(score_feature)\n",
    "            dynamic_ocr_scores = self.ocr_ptr_net(\n",
    "                score_feature, mmt_ocr_output, ocr_mask\n",
    "            )\n",
    "            scores = torch.cat([fixed_scores, dynamic_ocr_scores], dim=-1)\n",
    "        else:\n",
    "            dec_step_num = sample_list.train_prev_inds.size(1)\n",
    "            # fill prev_inds with BOS_IDX at index 0, and zeros elsewhere\n",
    "            prev_inds = torch.zeros_like(\n",
    "                sample_list.train_prev_inds\n",
    "            )\n",
    "            prev_inds[:, 0] = self.answer_processor.BOS_IDX\n",
    "            \n",
    "            # greedy decoding at test time\n",
    "            for t in range(dec_step_num):\n",
    "                mmt_results = self.mmt(\n",
    "                    s_o, so_mask, s_t, st_mask, \n",
    "                    g_o, go_mask, g_t, gt_mask, \n",
    "                    ocr_emb=ocr_emb,\n",
    "                    ocr_mask=ocr_mask,\n",
    "                    fixed_ans_emb=self.classifier.module.weight,\n",
    "                    prev_inds=prev_inds,\n",
    "                )\n",
    "                if t==0:\n",
    "                    g_O = mmt_results[\"mmt_so_output\"]*mmt_results[\"mmt_go_output\"]\n",
    "                    g_T = mmt_results[\"mmt_st_output\"]*mmt_results[\"mmt_gt_output\"]\n",
    "                    update_joint_embedding = torch.cat((g_O, g_T),dim=-1) # torch.Size([32, 1, 1536])\n",
    "                    update_joint_embedding = self.linear_joint(update_joint_embedding)\n",
    "                mmt_dec_output = mmt_results[\"mmt_dec_output\"]\n",
    "                score_feature = torch.cat([update_joint_embedding, mmt_dec_output[:,1:,:]], dim=-2)\n",
    "                mmt_ocr_output = mmt_results[\"mmt_ocr_output\"]\n",
    "                fixed_scores = self.classifier(score_feature)\n",
    "                dynamic_ocr_scores = self.ocr_ptr_net(\n",
    "                    score_feature, mmt_ocr_output, ocr_mask\n",
    "                )\n",
    "                scores = torch.cat([fixed_scores, dynamic_ocr_scores], dim=-1)\n",
    "                # find the highest scoring output (either a fixed vocab\n",
    "                # or an OCR), and add it to prev_inds for auto-regressive\n",
    "                # decoding\n",
    "                argmax_inds = scores.argmax(dim=-1)\n",
    "                prev_inds[:, 1:] = argmax_inds[:, :-1]\n",
    "\n",
    "        return {\"scores\": scores} \n",
    "\n",
    "class TextBert(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        # self.apply(self.init_weights)  # old versions of pytorch_transformers\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, txt_inds, txt_mask):\n",
    "        encoder_inputs = self.embeddings(txt_inds)\n",
    "        attention_mask = txt_mask\n",
    "\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        assert not extended_attention_mask.requires_grad\n",
    "        head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            encoder_inputs,\n",
    "            extended_attention_mask,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "        seq_output = encoder_outputs[0]\n",
    "\n",
    "        return seq_output\n",
    "\n",
    "class MMT(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.prev_pred_embeddings = PrevPredEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        # self.apply(self.init_weights)  # old versions of pytorch_transformers\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                so,so_mask,st,st_mask,\n",
    "                go,go_mask,gt,gt_mask,\n",
    "                ocr_emb,\n",
    "                ocr_mask,\n",
    "                fixed_ans_emb,\n",
    "                prev_inds):\n",
    "        # build embeddings for predictions in previous decoding steps\n",
    "        # fixed_ans_emb is an embedding lookup table for each fixed vocabulary\n",
    "        dec_emb = self.prev_pred_embeddings(fixed_ans_emb, ocr_emb, prev_inds)\n",
    "        # a zero mask for decoding steps, so the encoding steps elements can't\n",
    "        # attend to decoding steps.\n",
    "        # A triangular causal mask will be filled for the decoding steps\n",
    "        # later in extended_attention_mask\n",
    "        dec_mask = torch.zeros(dec_emb.size(0),dec_emb.size(1),dtype=torch.float32,device=dec_emb.device)\n",
    "        encoder_inputs = torch.cat(\n",
    "            [so,st,go,gt,ocr_emb,dec_emb],\n",
    "            dim=1\n",
    "        )\n",
    "        attention_mask = torch.cat(\n",
    "            [so_mask,st_mask,go_mask,gt_mask,ocr_mask,dec_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        # offsets of each modality in the joint embedding space\n",
    "        so_max_num = so_mask.size(-1)\n",
    "        st_max_num = st_mask.size(-1)\n",
    "        go_max_num = go_mask.size(-1)\n",
    "        gt_max_num = gt_mask.size(-1)\n",
    "        ocr_max_num = ocr_mask.size(-1)\n",
    "        dec_max_num = dec_mask.size(-1)\n",
    "        so_begin = 0\n",
    "        so_end = so_max_num\n",
    "        st_begin = so_max_num\n",
    "        st_end = st_begin + st_max_num\n",
    "        go_begin = so_max_num + st_max_num \n",
    "        go_end = go_begin + go_max_num\n",
    "        gt_begin = so_max_num  + st_max_num  + go_max_num \n",
    "        gt_end = gt_begin + gt_max_num\n",
    "        ocr_begin = so_max_num + st_max_num + go_max_num + gt_max_num\n",
    "        ocr_end = ocr_begin + ocr_max_num\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, from_seq_length, to_seq_length]\n",
    "        # So we can broadcast to\n",
    "        # [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        to_seq_length = attention_mask.size(1)\n",
    "        from_seq_length = to_seq_length\n",
    "\n",
    "        # generate the attention mask similar to prefix LM\n",
    "        # all elements can attend to the elements in encoding steps\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.repeat(\n",
    "            1, 1, from_seq_length, 1\n",
    "        )\n",
    "        # decoding step elements can attend to themselves in a causal manner\n",
    "        extended_attention_mask[:, :, -dec_max_num:, -dec_max_num:] = \\\n",
    "            _get_causal_mask(dec_max_num, encoder_inputs.device)\n",
    "\n",
    "        # flip the mask, so that invalid attention pairs have -10000.\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        assert not extended_attention_mask.requires_grad\n",
    "        head_mask = [None] * self.config.num_hidden_layers\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            encoder_inputs,\n",
    "            extended_attention_mask,\n",
    "            head_mask=head_mask\n",
    "        )\n",
    "\n",
    "        mmt_seq_output = encoder_outputs[0]\n",
    "        mmt_so_output = mmt_seq_output[:, so_begin:so_end]\n",
    "        mmt_st_output = mmt_seq_output[:, st_begin:st_end]\n",
    "        mmt_go_output = mmt_seq_output[:, go_begin:go_end]\n",
    "        mmt_gt_output = mmt_seq_output[:, gt_begin:gt_end]\n",
    "        mmt_ocr_output = mmt_seq_output[:, ocr_begin:ocr_end]\n",
    "        mmt_dec_output = mmt_seq_output[:, -dec_max_num:]\n",
    "\n",
    "        results = {\n",
    "            'mmt_seq_output': mmt_seq_output,\n",
    "            'mmt_so_output': mmt_so_output,\n",
    "            'mmt_st_output': mmt_st_output,\n",
    "            'mmt_go_output': mmt_go_output,\n",
    "            'mmt_gt_output': mmt_gt_output,\n",
    "            'mmt_ocr_output': mmt_ocr_output,\n",
    "            'mmt_dec_output': mmt_dec_output,\n",
    "        }\n",
    "        return results\n",
    "\n",
    "class OcrPtrNet(nn.Module):\n",
    "    def __init__(self, hidden_size, query_key_size=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if query_key_size is None:\n",
    "            query_key_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query_key_size = query_key_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, query_key_size)\n",
    "        self.key = nn.Linear(hidden_size, query_key_size)\n",
    "\n",
    "    def forward(self, query_inputs, key_inputs, attention_mask):\n",
    "        extended_attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "        assert extended_attention_mask.dim() == 2\n",
    "        extended_attention_mask = extended_attention_mask.unsqueeze(1)\n",
    "\n",
    "        query_layer = self.query(query_inputs)\n",
    "        if query_layer.dim() == 2:\n",
    "            query_layer = query_layer.unsqueeze(1)\n",
    "            squeeze_result = True\n",
    "        else:\n",
    "            squeeze_result = False\n",
    "        key_layer = self.key(key_inputs)\n",
    "\n",
    "        scores = torch.matmul(\n",
    "            query_layer,\n",
    "            key_layer.transpose(-1, -2)\n",
    "        )\n",
    "        scores = scores / math.sqrt(self.query_key_size)\n",
    "        scores = scores + extended_attention_mask\n",
    "        if squeeze_result:\n",
    "            scores = scores.squeeze(1)\n",
    "\n",
    "        return scores\n",
    "\n",
    "class PrevPredEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        MAX_DEC_LENGTH = 100\n",
    "        MAX_TYPE_NUM = 5\n",
    "        hidden_size = config.hidden_size\n",
    "        ln_eps = config.layer_norm_eps\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(MAX_DEC_LENGTH, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(MAX_TYPE_NUM, hidden_size)\n",
    "\n",
    "        self.ans_layer_norm = BertLayerNorm(hidden_size, eps=ln_eps)\n",
    "        self.ocr_layer_norm = BertLayerNorm(hidden_size, eps=ln_eps)\n",
    "        self.emb_layer_norm = BertLayerNorm(hidden_size, eps=ln_eps)\n",
    "        self.emb_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, ans_emb, ocr_emb, prev_inds):\n",
    "        assert prev_inds.dim() == 2 and prev_inds.dtype == torch.long\n",
    "        assert ans_emb.dim() == 2\n",
    "\n",
    "        batch_size = prev_inds.size(0)\n",
    "        seq_length = prev_inds.size(1)\n",
    "        ans_num = ans_emb.size(0)\n",
    "\n",
    "        # apply layer normalization to both answer embedding and OCR embedding\n",
    "        # before concatenation, so that they have the same scale\n",
    "        ans_emb = self.ans_layer_norm(ans_emb)\n",
    "        ocr_emb = self.ocr_layer_norm(ocr_emb)\n",
    "        assert ans_emb.size(-1) == ocr_emb.size(-1)\n",
    "        ans_emb = ans_emb.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        ans_ocr_emb_cat = torch.cat([ans_emb, ocr_emb], dim=1)\n",
    "        raw_dec_emb = _batch_gather(ans_ocr_emb_cat, prev_inds)\n",
    "\n",
    "        # Add position and type embedding for previous predictions\n",
    "        position_ids = torch.arange(\n",
    "            seq_length,\n",
    "            dtype=torch.long,\n",
    "            device=ocr_emb.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Token type ids: 0 -- vocab; 1 -- OCR\n",
    "        token_type_ids = prev_inds.ge(ans_num).long()\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = position_embeddings + token_type_embeddings\n",
    "        embeddings = self.emb_layer_norm(embeddings)\n",
    "        embeddings = self.emb_dropout(embeddings)\n",
    "        dec_emb = raw_dec_emb + embeddings\n",
    "\n",
    "        return dec_emb\n",
    "\n",
    "def _get_mask(nums, max_num):\n",
    "    # non_pad_mask: b x lq, torch.float32, 0. on PAD\n",
    "    batch_size = nums.size(0)\n",
    "    arange = torch.arange(0, max_num).unsqueeze(0).expand(batch_size, -1)\n",
    "    non_pad_mask = arange.to(nums.device).lt(nums.unsqueeze(-1))\n",
    "    non_pad_mask = non_pad_mask.type(torch.float32)\n",
    "    return non_pad_mask\n",
    "\n",
    "@functools.lru_cache(maxsize=32)\n",
    "def _get_causal_mask(seq_length, device):\n",
    "    # generate a lower triangular mask\n",
    "    mask = torch.zeros(seq_length, seq_length, device=device)\n",
    "    for i in range(seq_length):\n",
    "        for j in range(i+1):\n",
    "            mask[i, j] = 1.\n",
    "    return mask\n",
    "\n",
    "def _batch_gather(x, inds):\n",
    "    assert x.dim() == 3\n",
    "    batch_size = x.size(0)\n",
    "    length = x.size(1)\n",
    "    dim = x.size(2)\n",
    "    x_flat = x.view(batch_size*length, dim)\n",
    "\n",
    "    batch_offsets = torch.arange(batch_size, device=inds.device) * length\n",
    "    batch_offsets = batch_offsets.unsqueeze(-1)\n",
    "    assert batch_offsets.dim() == inds.dim()\n",
    "    inds_flat = batch_offsets + inds\n",
    "    results = F.embedding(inds_flat, x_flat)\n",
    "    return results\n",
    "\n",
    "def convert_prediction_to_ans(prediction_list, sample_texts, tokenizer):\n",
    "    raw_predicts = []\n",
    "\n",
    "    for l, text in zip(prediction_list, sample_texts):\n",
    "        raw_predict = []\n",
    "        vocab_predict = []\n",
    "\n",
    "        for w in l:\n",
    "            if w > 64000: # Kiểm tra nếu w là ocr token\n",
    "                if len(vocab_predict) > 0: # Kiểm tra nếu trước đó đã có các vocab token\n",
    "                    raw_predict.append(tokenizer.decode(vocab_predict))\n",
    "                    vocab_predict = []\n",
    "\n",
    "                raw_predict.append(text[w - 64001])\n",
    "            else:\n",
    "                vocab_predict += [w]\n",
    "\n",
    "        if len(vocab_predict) > 0:\n",
    "                raw_predict.append(tokenizer.decode(vocab_predict))\n",
    "\n",
    "        caption = ' '.join(raw_predict)\n",
    "        raw_predicts.append(caption)\n",
    "\n",
    "    return raw_predicts\n",
    "\n",
    "def ignore_padding(labels, outputs, padding_values):\n",
    "\n",
    "  mask = labels != padding_values\n",
    "\n",
    "  new_outputs = []\n",
    "\n",
    "  for i, each in enumerate(mask) :\n",
    "    ignore_outputs = outputs[i][each].tolist()\n",
    "    new_outputs.append(ignore_outputs)\n",
    "\n",
    "  return new_outputs\n",
    "\n",
    "def compute_metrics(outputs, labels, padding_values, raw_captions, ocr_tokens, tokenizer):\n",
    "    batch_size, seq_length, vocab_size = outputs.size()\n",
    "\n",
    "    outputs_c = outputs.argmax(dim=-1)\n",
    "    outputs_c = ignore_padding(labels, outputs_c, padding_values)\n",
    "\n",
    "    pred_ans = convert_prediction_to_ans(outputs_c, ocr_tokens, tokenizer)\n",
    "\n",
    "    check = np.random.randn()\n",
    "    if check > 0.7:\n",
    "        print('-'*30)\n",
    "        print(f'prediction: {pred_ans[0]}')\n",
    "        print(f'ground-tru: {raw_captions[0][0]}')\n",
    "\n",
    "    bleu1_score = bleu.compute(predictions=pred_ans, references=raw_captions, max_order=1)['bleu']\n",
    "    bleu2_score = bleu.compute(predictions=pred_ans, references=raw_captions, max_order=2)['bleu']\n",
    "    bleu3_score = bleu.compute(predictions=pred_ans, references=raw_captions, max_order=3)['bleu']\n",
    "    bleu4_score = bleu.compute(predictions=pred_ans, references=raw_captions, max_order=4)['bleu']\n",
    "    rouge_score = rouge.compute(predictions=pred_ans, references=raw_captions)['rougeL']\n",
    "    meteor_score = meteor.compute(predictions=pred_ans, references=raw_captions)['meteor']\n",
    "\n",
    "    hypotheses_dict = {i: [h] for i, h in enumerate(pred_ans)}\n",
    "    references_dict = {i: r for i, r in enumerate(raw_captions)}\n",
    "\n",
    "\n",
    "    # Compute the CIDEr score\n",
    "    cider_score, _ = cider_scorer.compute_score(references_dict, hypotheses_dict)\n",
    "\n",
    "    outputs_1 = outputs.view(-1, vocab_size)\n",
    "    labels_1 = labels.view(-1)\n",
    "    mask = labels_1 != padding_values\n",
    "\n",
    "    outputs_2 = outputs_1[mask]\n",
    "    labels_2 = labels_1[mask]\n",
    "\n",
    "    acc = (outputs_2.argmax(dim=-1) == labels_2).sum().item() / len(labels_2)\n",
    "\n",
    "    return [acc, bleu1_score, bleu2_score, bleu3_score, bleu4_score, rouge_score, meteor_score, cider_score]\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, criterion, optimizer, tokenizer, epochs, device='cpu'):\n",
    "\n",
    "  # Send model to device\n",
    "  model = model.to(device)\n",
    "\n",
    "  train_losses, test_losses = [], []\n",
    "  train_evals, test_evals = {'accuracy': [], 'bleu1': [], 'bleu2': [], 'bleu3': [], 'bleu4': [], 'rouge': [], 'meteor': [], 'cider': []}, {'accuracy': [], 'bleu1': [], 'bleu2': [], 'bleu3': [], 'bleu4': [], 'rouge': [], 'meteor': [], 'cider': []}\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    train_loss, test_loss = 0, 0\n",
    "    train_eval, test_eval = [], []\n",
    "    i = 0\n",
    "    k = len(train_dataloader)\n",
    "\n",
    "    ### Train ###\n",
    "    model.train()\n",
    "\n",
    "    for sample in tqdm(train_dataloader):\n",
    "\n",
    "      # Forwad pass\n",
    "      outputs = model(sample, device) # batch_size, T, vocab + ocr_tokens\n",
    "      loss = criterion(outputs.mT, sample['labels'].to(device))\n",
    "      optimizer.zero_grad() # Xóa cái optimizer ở vòng lặp trước\n",
    "\n",
    "      # Calculate loss per batch\n",
    "      train_loss += loss.item()\n",
    "      train_eval.append(compute_metrics(outputs, sample['labels'].to(device), 1, sample['raw_captions'], sample['texts'], tokenizer))\n",
    "\n",
    "      # Optimizer & Backward\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "      optimizer.step() # update weight\n",
    "      # scheduler.step()\n",
    "      i += 1\n",
    "      if i == round(k * 0.8):\n",
    "        for g in optimizer.param_groups:\n",
    "          g['lr'] *= 0.95\n",
    "\n",
    "\n",
    "    ### Evaluate ###\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "      for sample in tqdm(test_dataloader):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sample, device) # batch_size, T, vocab + ocr_tokens\n",
    "        loss = criterion(outputs.mT, sample['labels'].to(device))\n",
    "\n",
    "        # Calculate loss per batch\n",
    "        test_loss += loss.item()\n",
    "        test_eval.append(compute_metrics(outputs, sample['labels'].to(device), 1, sample['raw_captions'], sample['texts'], tokenizer))\n",
    "\n",
    "    # Preprocess stuff\n",
    "    train_eval, test_eval = np.array(train_eval), np.array(test_eval)\n",
    "\n",
    "\n",
    "    # Save checkpoints\n",
    "    if epoch % 5 == 0:\n",
    "        PATH = f\"model_epoch_{epoch}.pt\"  # Save with epoch number\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),  # Save only the last loss\n",
    "        }, PATH)\n",
    "\n",
    "    # Save stuff\n",
    "    test_loss, train_loss = test_loss/len(test_dataloader), train_loss/len(train_dataloader)\n",
    "    train_acc, test_acc = train_eval[:, 0].mean(), test_eval[:, 0].mean()\n",
    "    train_bleu1, test_bleu1 = train_eval[:, 1].mean(), test_eval[:, 1].mean()\n",
    "    train_bleu2, test_bleu2 = train_eval[:, 2].mean(), test_eval[:, 2].mean()\n",
    "    train_bleu3, test_bleu3 = train_eval[:, 3].mean(), test_eval[:, 3].mean()\n",
    "    train_bleu4, test_bleu4 = train_eval[:, 4].mean(), test_eval[:, 4].mean()\n",
    "    train_rouge, test_rouge = train_eval[:, 5].mean(), test_eval[:, 5].mean()\n",
    "    train_meteor, test_meteor = train_eval[:, 6].mean(), test_eval[:, 6].mean()\n",
    "    train_cider, test_cider = train_eval[:, 7].mean(), test_eval[:, 7].mean()\n",
    "\n",
    "\n",
    "    train_losses.append(train_loss), test_losses.append(test_loss)\n",
    "    train_evals['accuracy'].append(train_acc), test_evals['accuracy'].append(test_acc)\n",
    "    train_evals['bleu1'].append(train_bleu1), test_evals['bleu1'].append(test_bleu1)\n",
    "    train_evals['bleu2'].append(train_bleu2), test_evals['bleu2'].append(test_bleu2)\n",
    "    train_evals['bleu3'].append(train_bleu3), test_evals['bleu3'].append(test_bleu3)\n",
    "    train_evals['bleu4'].append(train_bleu4), test_evals['bleu4'].append(test_bleu4)\n",
    "    train_evals['rouge'].append(train_rouge), test_evals['rouge'].append(test_rouge)\n",
    "    train_evals['meteor'].append(train_meteor), test_evals['meteor'].append(test_meteor)\n",
    "    train_evals['cider'].append(train_cider), test_evals['cider'].append(test_cider)\n",
    "\n",
    "    # Tracking the model\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'Train loss    : {train_loss:.4f}  | Test loss   : {test_loss:.4f}')\n",
    "    print(f'Train acc     : {train_acc:.4f}   | Test acc    : {test_acc:.4f}')\n",
    "    print(f'Train bleu1   : {train_bleu1:.4f} | Test bleu1  : {test_bleu1:.4f}')\n",
    "    print(f'Train bleu2   : {train_bleu2:.4f} | Test bleu2  : {test_bleu2:.4f}')\n",
    "    print(f'Train bleu3   : {train_bleu3:.4f} | Test bleu3  : {test_bleu3:.4f}')\n",
    "    print(f'Train bleu4   : {train_bleu4:.4f} | Test bleu4  : {test_bleu4:.4f}')\n",
    "    print(f'Train rouge   : {train_rouge:.4f} | Test rouge  : {test_rouge:.4f}')\n",
    "    print(f'Train meteor  : {train_meteor:.4f}| Test meteor : {test_meteor:.4f}')\n",
    "    print(f'Train cider   : {train_cider:.4f} | Test cider  : {test_cider:.4f}')\n",
    "\n",
    "  return train_losses, train_evals, test_losses, test_evals\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(outputs, labels, padding_value):\n",
    "\n",
    "  vocab_ocr_size = outputs.size(-1)\n",
    "  outputs_ = outputs.reshape(-1, vocab_ocr_size) # batch_size * seq_l, vocab + ocr size\n",
    "  labels_ = labels.reshape(-1) # batch_size * seq_l\n",
    "\n",
    "  mask = labels_ != padding_value\n",
    "\n",
    "  outputs_non_pad = outputs_[mask] # batch_size * seq_l - pad, vocab + ocr size\n",
    "  labels_non_pad = labels_[mask] # batch_size * seq_l - pad\n",
    "\n",
    "  # Use the original tensor as indices to select rows from the identity matrix\n",
    "  converted_labels = torch.zeros_like(outputs_non_pad) # batch_size * seq_l - pad, vocab + ocr size\n",
    "  converted_labels[torch.arange(len(labels_non_pad)), labels_non_pad] = 1\n",
    "\n",
    "  pos_weight = (converted_labels==0.).sum()/converted_labels.sum()\n",
    "  criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='sum')\n",
    "\n",
    "  return criterion(outputs_non_pad, converted_labels)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "phobert_model.embeddings.word_embeddings.requires_grad = False\n",
    "fixed_ans_emb = phobert_model.embeddings.word_embeddings.weight\n",
    "model = M4C(obj_in_dim=1024,\n",
    "            ocr_in_dim=812,\n",
    "            hidden_size=768,\n",
    "            n_heads=12,\n",
    "            d_k=64,\n",
    "            n_layers=4,\n",
    "            vocab_size=tokenizer.vocab_size + 1,\n",
    "            fixed_ans_emb=fixed_ans_emb)\n",
    "model.apply(initialize_weights);\n",
    "loss_function = lambda outputs, labels: loss_fn(outputs, labels, padding_value=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# Learning rate warm-up function\n",
    "warmup = 400\n",
    "model_size = 768\n",
    "factor = 0.1\n",
    "def warmup_lr(current_epoch):\n",
    "    current_epoch += 1\n",
    "    return factor * (model_size ** (-0.5) * min(current_epoch ** (-0.5), current_epoch * warmup ** (-1.5)))\n",
    "\n",
    "# Create a LambdaLR scheduler for learning rate warm-up\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1, 20000), [warmup_lr(i) for i in range(1, 20000)])\n",
    "\n",
    "for epoch in range(1, 25):\n",
    "    scheduler.step()\n",
    "    print('Epoch {}, lr {}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "base_lr = 1e-4\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = torch.optim.Adam(model.get_optimizer_parameters(base_lr), lr=base_lr)\n",
    "\n",
    "train_losses, train_accies, test_losses, test_accies = train(model,\n",
    "                                                             train_dataloader,\n",
    "                                                             dev_dataloader,\n",
    "                                                             criterion,\n",
    "                                                             optimizer,\n",
    "                                                             tokenizer,\n",
    "                                                             epochs=20,\n",
    "                                                             device=device)\n",
    "\n",
    "checkpoint = {'model': M4C(obj_in_dim=1024,\n",
    "            ocr_in_dim=812,\n",
    "            hidden_size=768,\n",
    "            n_heads=12,\n",
    "            d_k=64,\n",
    "            n_layers=4,\n",
    "            vocab_size=tokenizer.vocab_size + 1,\n",
    "            fixed_ans_emb=fixed_ans_emb),\n",
    "              'state_dict': model.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "# Generate x-axis values (epochs)\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "def plot_(epochs, train_losses):\n",
    "    # Plot the training losses\n",
    "    plt.plot(epochs, train_losses, marker='o', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_(epochs, train_losses)\n",
    "plot_(epochs, train_accies)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "epochs = range(1, len(train_accies['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, train_accies['accuracy'], 'b', label='Accuracy')\n",
    "\n",
    "plt.plot(epochs, train_accies['bleu1'], 'g', label='BLEU-1')\n",
    "plt.plot(epochs, train_accies['bleu2'], 'r', label='BLEU-2')\n",
    "plt.plot(epochs, train_accies['bleu3'], 'c', label='BLEU-3')\n",
    "plt.plot(epochs, train_accies['bleu4'], 'm', label='BLEU-4')\n",
    "plt.plot(epochs, train_accies['rouge'], 'y', label='ROUGE')\n",
    "plt.plot(epochs, train_accies['meteor'], 'k', label='METEOR')\n",
    "plt.plot(epochs, train_accies['cider'], 'orange', label='CIDEr')\n",
    "\n",
    "plt.title('TRAINING - Metrics Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "epochs = range(1, len(test_accies['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(epochs, test_accies['accuracy'], 'b', label='Accuracy')\n",
    "\n",
    "plt.plot(epochs, test_accies['bleu1'], 'g', label='BLEU-1')\n",
    "plt.plot(epochs, test_accies['bleu2'], 'r', label='BLEU-2')\n",
    "plt.plot(epochs, test_accies['bleu3'], 'c', label='BLEU-3')\n",
    "plt.plot(epochs, test_accies['bleu4'], 'm', label='BLEU-4')\n",
    "plt.plot(epochs, test_accies['rouge'], 'y', label='ROUGE')\n",
    "plt.plot(epochs, test_accies['meteor'], 'k', label='METEOR')\n",
    "plt.plot(epochs, test_accies['cider'], 'orange', label='CIDEr')\n",
    "\n",
    "plt.title('VALIDATION - Metrics Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def test_fn(model, test_dataloader, criterion, optimizer, tokenizer, device='cpu'):\n",
    "    model.to(device)\n",
    "    test_eval = []\n",
    "    test_evals = {'accuracy': [], 'bleu1': [], 'bleu2': [], 'bleu3': [], 'bleu4': [], 'rouge': [], 'meteor': [], 'cider': []}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    with torch.inference_mode():\n",
    "\n",
    "      for sample in tqdm(test_dataloader):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sample, device, test=True) # batch_size, T, vocab + ocr_tokens\n",
    "        loss = criterion(outputs.mT, sample['labels'].to(device))\n",
    "\n",
    "        # Calculate loss per batch\n",
    "        test_loss += loss.item()\n",
    "        test_eval.append(compute_metrics(outputs, sample['labels'].to(device), 1, sample['raw_captions'], sample['texts'], tokenizer))\n",
    "\n",
    "    test_eval = np.array(test_eval)\n",
    "\n",
    "    # Save stuff\n",
    "    test_loss = test_loss/len(test_dataloader)\n",
    "    test_acc =  test_eval[:, 0].mean()\n",
    "    test_bleu1 = test_eval[:, 1].mean()\n",
    "    test_bleu2 = test_eval[:, 2].mean()\n",
    "    test_bleu3 = test_eval[:, 3].mean()\n",
    "    test_bleu4 = test_eval[:, 4].mean()\n",
    "    test_rouge = test_eval[:, 5].mean()\n",
    "    test_meteor = test_eval[:, 6].mean()\n",
    "    test_cider = test_eval[:, 7].mean()\n",
    "\n",
    "\n",
    "    test_evals['accuracy'].append(test_acc)\n",
    "    test_evals['bleu1'].append(test_bleu1)\n",
    "    test_evals['bleu2'].append(test_bleu2)\n",
    "    test_evals['bleu3'].append(test_bleu3)\n",
    "    test_evals['bleu4'].append(test_bleu4)\n",
    "    test_evals['rouge'].append(test_rouge)\n",
    "    test_evals['meteor'].append(test_meteor)\n",
    "    test_evals['cider'].append(test_cider)\n",
    "\n",
    "    # Tracking the model\n",
    "\n",
    "    print(f'Test loss   : {test_loss:.4f}')\n",
    "    print(f'Test acc    : {test_acc:.4f}')\n",
    "    print(f'Test bleu1  : {test_bleu1:.4f}')\n",
    "    print(f'Test bleu2  : {test_bleu2:.4f}')\n",
    "    print(f'Test bleu3  : {test_bleu3:.4f}')\n",
    "    print(f'Test bleu4  : {test_bleu4:.4f}')\n",
    "    print(f'Test rouge  : {test_rouge:.4f}')\n",
    "    print(f'Test meteor : {test_meteor:.4f}')\n",
    "    print(f'Test cider  : {test_cider:.4f}')\n",
    "\n",
    "    return test_loss, test_evals\n",
    "\n",
    "model = model.to(device)\n",
    "test_fn(model, test_dataloader, criterion, optimizer, tokenizer, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pythia(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self._global_config = registry.get(\"config\")\n",
    "        self._datasets = self._global_config.datasets.split(\",\")\n",
    "\n",
    "    def build(self):\n",
    "        self._build_word_embedding()\n",
    "        self._init_text_embeddings(\"text\")\n",
    "        self._init_feature_encoders(\"image\")\n",
    "        self._init_feature_embeddings(\"image\")\n",
    "        self._init_combine_layer(\"image\", \"text\")\n",
    "        self._init_classifier(self._get_classifier_input_dim())\n",
    "        self._init_extras()\n",
    "\n",
    "    def _build_word_embedding(self):\n",
    "        assert len(self._datasets) > 0\n",
    "        text_processor = registry.get(self._datasets[0] + \"_text_processor\")\n",
    "        vocab = text_processor.vocab\n",
    "        self.word_embedding = vocab.get_embedding(torch.nn.Embedding, embedding_dim=300)\n",
    "\n",
    "    def _init_text_embeddings(self, attr=\"text\"):\n",
    "        if \"embeddings\" not in attr:\n",
    "            attr += \"_embeddings\"\n",
    "\n",
    "        text_embeddings = []\n",
    "        text_embeddings_list_config = self.config[attr]\n",
    "\n",
    "        embeddings_out_dim = 0\n",
    "\n",
    "        for text_embedding in text_embeddings_list_config:\n",
    "            embedding_type = text_embedding.type\n",
    "            embedding_kwargs = ConfigNode(text_embedding.params)\n",
    "\n",
    "            self._update_text_embedding_args(embedding_kwargs)\n",
    "\n",
    "            embedding = TextEmbedding(embedding_type, **embedding_kwargs)\n",
    "\n",
    "            text_embeddings.append(embedding)\n",
    "            embeddings_out_dim += embedding.text_out_dim\n",
    "\n",
    "        setattr(self, attr + \"_out_dim\", embeddings_out_dim)\n",
    "        setattr(self, attr, nn.ModuleList(text_embeddings))\n",
    "\n",
    "    def _update_text_embedding_args(self, args):\n",
    "        # Add model_data_dir to kwargs\n",
    "        args[\"model_data_dir\"] = self.config[\"model_data_dir\"]\n",
    "\n",
    "    def _init_feature_encoders(self, attr):\n",
    "        feat_encoders = []\n",
    "        feat_encoders_list_config = self.config[attr + \"_feature_encodings\"]\n",
    "        feature_dim = self.config[attr + \"_feature_dim\"]\n",
    "        setattr(self, attr + \"_feature_dim\", feature_dim)\n",
    "\n",
    "        for feat_encoder in feat_encoders_list_config:\n",
    "            encoder_type = feat_encoder[\"type\"]\n",
    "            encoder_kwargs = feat_encoder[\"params\"]\n",
    "            encoder_kwargs[\"model_data_dir\"] = self.config[\"model_data_dir\"]\n",
    "\n",
    "            feat_model = ImageEncoder(encoder_type, feature_dim, **encoder_kwargs)\n",
    "\n",
    "            feat_encoders.append(feat_model)\n",
    "            setattr(self, attr + \"_feature_dim\", feat_model.out_dim)\n",
    "\n",
    "        setattr(self, attr + \"_feature_encoders\", nn.ModuleList(feat_encoders))\n",
    "\n",
    "    def _init_feature_embeddings(self, attr):\n",
    "        feature_embeddings_list = []\n",
    "        num_feature_feat = len(\n",
    "            getattr(self.config, \"{}_feature_encodings\".format(attr))\n",
    "        )\n",
    "\n",
    "        self.feature_embeddings_out_dim = 0\n",
    "\n",
    "        for _ in range(num_feature_feat):\n",
    "            feature_embeddings = []\n",
    "            feature_attn_model_list = self.config[attr + \"_feature_embeddings\"]\n",
    "\n",
    "            for feature_attn_model_params in feature_attn_model_list:\n",
    "                feature_embedding = ImageEmbedding(\n",
    "                    getattr(self, attr + \"_feature_dim\"),\n",
    "                    self.text_embeddings_out_dim,\n",
    "                    **feature_attn_model_params\n",
    "                )\n",
    "                feature_embeddings.append(feature_embedding)\n",
    "                self.feature_embeddings_out_dim += feature_embedding.out_dim\n",
    "\n",
    "            feature_embeddings = nn.ModuleList(feature_embeddings)\n",
    "            feature_embeddings_list.append(feature_embeddings)\n",
    "\n",
    "        self.feature_embeddings_out_dim *= getattr(self, attr + \"_feature_dim\")\n",
    "\n",
    "        setattr(\n",
    "            self, attr + \"_feature_embeddings_out_dim\", self.feature_embeddings_out_dim\n",
    "        )\n",
    "        del self.feature_embeddings_out_dim\n",
    "        setattr(\n",
    "            self,\n",
    "            attr + \"_feature_embeddings_list\",\n",
    "            nn.ModuleList(feature_embeddings_list),\n",
    "        )\n",
    "\n",
    "    def _get_embeddings_attr(self, attr):\n",
    "        embedding_attr1 = attr\n",
    "        if hasattr(self, attr + \"_embeddings_out_dim\"):\n",
    "            embedding_attr1 = attr + \"_embeddings_out_dim\"\n",
    "        else:\n",
    "            embedding_attr1 = attr + \"_feature_embeddings_out_dim\"\n",
    "\n",
    "        return embedding_attr1\n",
    "\n",
    "    def _init_combine_layer(self, attr1, attr2):\n",
    "        config_attr = attr1 + \"_\" + attr2 + \"_modal_combine\"\n",
    "\n",
    "        multi_modal_combine_layer = ModalCombineLayer(\n",
    "            self.config[config_attr][\"type\"],\n",
    "            getattr(self, self._get_embeddings_attr(attr1)),\n",
    "            getattr(self, self._get_embeddings_attr(attr2)),\n",
    "            **self.config[config_attr][\"params\"]\n",
    "        )\n",
    "\n",
    "        setattr(\n",
    "            self,\n",
    "            attr1 + \"_\" + attr2 + \"_multi_modal_combine_layer\",\n",
    "            multi_modal_combine_layer,\n",
    "        )\n",
    "\n",
    "    def _init_classifier(self, combined_embedding_dim):\n",
    "        # TODO: Later support multihead\n",
    "        num_choices = registry.get(self._datasets[0] + \"_num_final_outputs\")\n",
    "\n",
    "        self.classifier = ClassifierLayer(\n",
    "            self.config[\"classifier\"][\"type\"],\n",
    "            in_dim=combined_embedding_dim,\n",
    "            out_dim=num_choices,\n",
    "            **self.config[\"classifier\"][\"params\"]\n",
    "        )\n",
    "\n",
    "    def _init_extras(self):\n",
    "        self.inter_model = None\n",
    "\n",
    "    def get_optimizer_parameters(self, config):\n",
    "        combine_layer = self.image_text_multi_modal_combine_layer\n",
    "        params = [\n",
    "            {\"params\": self.word_embedding.parameters()},\n",
    "            {\"params\": self.image_feature_embeddings_list.parameters()},\n",
    "            {\"params\": self.text_embeddings.parameters()},\n",
    "            {\"params\": combine_layer.parameters()},\n",
    "            {\"params\": self.classifier.parameters()},\n",
    "            {\n",
    "                \"params\": self.image_feature_encoders.parameters(),\n",
    "                \"lr\": (config[\"optimizer_attributes\"][\"params\"][\"lr\"] * 0.1),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _get_classifier_input_dim(self):\n",
    "        return self.image_text_multi_modal_combine_layer.out_dim\n",
    "\n",
    "    def process_text_embedding(\n",
    "        self, sample_list, embedding_attr=\"text_embeddings\", info=None\n",
    "    ):\n",
    "        text_embeddings = []\n",
    "\n",
    "        # Get \"text\" attribute in case of \"text_embeddings\" case\n",
    "        # and \"context\" attribute in case of \"context_embeddings\"\n",
    "        texts = getattr(sample_list, embedding_attr.split(\"_\")[0])\n",
    "\n",
    "        # Get embedding models\n",
    "        text_embedding_models = getattr(self, embedding_attr)\n",
    "\n",
    "        for text_embedding_model in text_embedding_models:\n",
    "            # TODO: Move this logic inside\n",
    "            if isinstance(text_embedding_model, PreExtractedEmbedding):\n",
    "                embedding = text_embedding_model(sample_list.question_id)\n",
    "            else:\n",
    "                embedding = text_embedding_model(texts)\n",
    "            text_embeddings.append(embedding)\n",
    "\n",
    "        # # visualize decomposed question attention\n",
    "        # image_id = getattr(sample_list, \"image_id\")\n",
    "        # question_id = getattr(sample_list, \"question_id\").cpu()\n",
    "        # question_id = question_id.numpy()\n",
    "        # batch_size_t, _, _ = text_embeddings[0][7].shape\n",
    "        # for cnt in range(0, batch_size_t):\n",
    "        #     # image_path_org = './save/temp_check/'+question_id[cnt]+'image_id.pdh'\n",
    "        #     # torch.save(image_id[cnt], image_path_org)\n",
    "        #     attn_path_org = './save/temp_check/'+str(question_id[cnt])+'_a_o.pdh'\n",
    "        #     torch.save(text_embeddings[0][7][cnt], attn_path_org)\n",
    "        #     attn_path_org = './save/temp_check/'+str(question_id[cnt])+'_a_oo.pdh'\n",
    "        #     torch.save(text_embeddings[0][8][cnt], attn_path_org)\n",
    "        #     attn_path_org = './save/temp_check/'+str(question_id[cnt])+'_a_ot.pdh'\n",
    "        #     torch.save(text_embeddings[0][9][cnt], attn_path_org)\n",
    "        #     attn_path_org = './save/temp_check/'+str(question_id[cnt])+'_a_t.pdh'\n",
    "        #     torch.save(text_embeddings[0][10][cnt], attn_path_org)\n",
    "        #     attn_path_org = './save/temp_check/'+str(question_id[cnt])+'_a_tt.pdh'\n",
    "        #     torch.save(text_embeddings[0][11][cnt], attn_path_org)\n",
    "        #     attn_path_org = './save/temp_check/'+str(question_id[cnt])+'_a_to.pdh'\n",
    "        #     torch.save(text_embeddings[0][12][cnt], attn_path_org)\n",
    "        return text_embeddings[0][0], text_embeddings[0][1], text_embeddings[0][2], text_embeddings[0][3], text_embeddings[0][4], text_embeddings[0][5], text_embeddings[0][6]\n",
    "\n",
    "    def process_feature_embedding(\n",
    "        self, attr, sample_list, s_central, \n",
    "        s_homo=None, s_hetero=None, pre_ques_embed=None,\n",
    "        obj_feats=None, ocr_feats=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "\n",
    "        input: \n",
    "        attr: \"image\" or \"context\"\n",
    "        sample_list: just sample_list\n",
    "        s_central: question features for guiding purpose, torch.Size([128, 2048])\n",
    "                   s_o/s_t\n",
    "        s_homo: s_oo/s_tt\n",
    "        s_hetero: s_ot/s_to\n",
    "\n",
    "        output:\n",
    "        \"\"\"\n",
    "        # add obj bbox feats and image size        \n",
    "        batch, bbox_num, obj_feat_dim = obj_feats.shape\n",
    "        _, _, ocr_feat_dim = ocr_feats.shape\n",
    "        knn_k = 5\n",
    "        loc_dim = 5\n",
    "        # expand obj_feats\n",
    "        temp_expand_obj_feat = obj_feats[0][0]\n",
    "        temp_expand_obj_feat = temp_expand_obj_feat.expand(batch,1,obj_feat_dim)*0\n",
    "        temp_expand_obj_feat = torch.cat((obj_feats,temp_expand_obj_feat),1)\n",
    "                    \n",
    "        # expand ocr_feats\n",
    "        temp_expand_ocr_feat = ocr_feats[0][0]\n",
    "        temp_expand_ocr_feat = temp_expand_ocr_feat.expand(batch,1,ocr_feat_dim)*0\n",
    "        temp_expand_ocr_feat = torch.cat((ocr_feats,temp_expand_ocr_feat),1)\n",
    "       \n",
    "        if attr == 'image':\n",
    "            batch_size_t = ( sample_list.get_batch_size() )\n",
    "            # Get \"image_feature_0\"\n",
    "            feature = getattr(\n",
    "                sample_list, \"{}_feature_{:d}\".format(attr, 0), None\n",
    "            )\n",
    "            feature = feature[:batch_size_t]\n",
    "            # Get info related to the current feature. info is generally\n",
    "            # in key of format \"image_info_0\" for 0th feature\n",
    "            feature_info = getattr(sample_list, \"{}_info_{:d}\".format(attr, 0), {})\n",
    "            # For Pythia, we need max_features to mask attention\n",
    "            feature_dim = getattr(feature_info, \"max_features\", None)\n",
    "            if feature_dim is not None:\n",
    "                feature_dim = feature_dim[:batch_size_t]\n",
    "            # Get feature embedding\n",
    "            feature_embedding_model = getattr(self, attr + \"_feature_embedding\")\n",
    "            encoded_feature = obj_feats\n",
    "            batch, bbox_num, obj_feat_dim = encoded_feature.shape\n",
    "\n",
    "            # obj_obj_edge_feature = None\n",
    "            # oo edge generation\n",
    "            obj_obj_edge_feature = torch.zeros((batch, bbox_num, knn_k, obj_feat_dim+loc_dim)).float()\n",
    "            obj_obj_edge_feature = obj_obj_edge_feature.cuda()\n",
    "            oo_edge = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_oo\")\n",
    "            oo_edgefeats = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_oofeats\")\n",
    "            for i in range (batch):\n",
    "                obj_obj_edge_feature[i] = torch.cat((oo_edgefeats[i], temp_expand_obj_feat[i][oo_edge[i]]),2)\n",
    "            \n",
    "            # obj_text_edge_feature = None\n",
    "            # ot edge generation\n",
    "            obj_text_edge_feature = torch.zeros((batch, bbox_num, knn_k, ocr_feat_dim+loc_dim)).float()\n",
    "            obj_text_edge_feature = obj_text_edge_feature.cuda()\n",
    "            ot_edge = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_ot\")\n",
    "            ot_edgefeats = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_otfeats\")\n",
    "            for i in range (batch):\n",
    "                obj_text_edge_feature[i] = torch.cat((ot_edgefeats[i], temp_expand_ocr_feat[i][ot_edge[i]]),2)\n",
    "\n",
    "            oo_edge_feature = obj_obj_edge_feature\n",
    "            ot_edge_feature = obj_text_edge_feature\n",
    "            \n",
    "            s_o, s_oo, s_ot = s_central, s_homo, s_hetero\n",
    "            # for ablation study purpose, \n",
    "            # o feature + oo relation + ot relation\n",
    "            if (s_oo is not None) and (oo_edge_feature is not None) and (s_ot is not None) and (ot_edge_feature is not None) and (pre_ques_embed is not None):\n",
    "                inp = (attr, encoded_feature, s_o, feature_dim, s_oo, oo_edge_feature, s_ot, ot_edge_feature,pre_ques_embed)\n",
    "            # o feature + oo relation\n",
    "            elif (s_oo is not None) and (oo_edge_feature is not None) and (pre_ques_embed is not None):\n",
    "                inp = (attr, encoded_feature, s_o, feature_dim, s_oo, oo_edge_feature, pre_ques_embed)\n",
    "            # o feature + ot relation\n",
    "            elif (s_ot is not None) and (ot_edge_feature is not None) and (pre_ques_embed is not None):\n",
    "                inp = (attr, encoded_feature, s_o, feature_dim, s_ot, ot_edge_feature,pre_ques_embed)\n",
    "            # o feature only\n",
    "            else: inp = (attr, encoded_feature, s_o, feature_dim)\n",
    "            \n",
    "            g_o = feature_embedding_model(*inp)\n",
    "            return g_o\n",
    "\n",
    "        elif attr == 'context':\n",
    "            batch_size_t = ( sample_list.get_batch_size() )\n",
    "            # Get \"context_feature_0\"\n",
    "            feature = getattr(\n",
    "                sample_list, \"{}_feature_{:d}\".format(attr, 0), None\n",
    "            )\n",
    "            feature = feature[:batch_size_t]\n",
    "            # Get info related to the current feature. info is generally\n",
    "            # in key of format \"image_info_0\" for 0th feature\n",
    "            feature_info = getattr(sample_list, \"{}_info_{:d}\".format(attr, 0), {})\n",
    "            # For Pythia, we need max_features to mask attention\n",
    "            feature_dim = getattr(feature_info, \"max_features\", None)\n",
    "            if feature_dim is not None:\n",
    "                feature_dim = feature_dim[:batch_size_t]\n",
    "            # Get feature embedding\n",
    "            feature_embedding_model = getattr(self, \"context_feature_embedding\")\n",
    "            encoded_feature = ocr_feats\n",
    "            batch, bbox_num, _ = encoded_feature.shape\n",
    "            \n",
    "            # text_text_edge_feature = None\n",
    "            # tt edge generation\n",
    "            text_text_edge_feature = torch.zeros((batch, bbox_num, knn_k, ocr_feat_dim+loc_dim)).float()\n",
    "            text_text_edge_feature = text_text_edge_feature.cuda()\n",
    "            tt_edge = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_tt\")\n",
    "            tt_edgefeats = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_ttfeats\")\n",
    "            for i in range (batch):\n",
    "                text_text_edge_feature[i] = torch.cat((tt_edgefeats[i], temp_expand_ocr_feat[i][tt_edge[i]]),2)\n",
    "\n",
    "            # text_obj_edge_feature = None\n",
    "            # to edge generation\n",
    "            text_obj_edge_feature = torch.zeros((batch, bbox_num, knn_k, obj_feat_dim+loc_dim)).float()\n",
    "            text_obj_edge_feature = text_obj_edge_feature.cuda()\n",
    "            to_edge = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_to\")\n",
    "            to_edgefeats = getattr(getattr(sample_list, \"ocr_bbox\"), \"edge_tofeats\")\n",
    "            for i in range (batch):\n",
    "                text_obj_edge_feature[i] = torch.cat((to_edgefeats[i], temp_expand_obj_feat[i][to_edge[i]]),2)\n",
    "\n",
    "            tt_edge_feature = text_text_edge_feature\n",
    "            to_edge_feature = text_obj_edge_feature\n",
    "            \n",
    "            s_t, s_tt, s_to = s_central, s_homo, s_hetero\n",
    "            # for ablation study purpose\n",
    "            # t feature + tt relation + to relation\n",
    "            if (s_tt is not None) and (tt_edge_feature is not None) and (s_to is not None) and (to_edge_feature is not None) and (pre_ques_embed is not None):\n",
    "                inp = (attr, encoded_feature, s_t, feature_dim, s_tt, tt_edge_feature, s_to, to_edge_feature,pre_ques_embed)\n",
    "            # t feature + tt relation\n",
    "            elif (s_tt is not None) and (tt_edge_feature is not None) and (pre_ques_embed is not None):\n",
    "                inp = (attr, encoded_feature, s_t, feature_dim, s_tt, tt_edge_feature, pre_ques_embed)\n",
    "            # t feature + to relation\n",
    "            elif (s_to is not None) and (to_edge_feature is not None) and (pre_ques_embed is not None):\n",
    "                inp = (attr, encoded_feature, s_t, feature_dim, s_to, to_edge_feature,pre_ques_embed)\n",
    "            # t feature only\n",
    "            else:\n",
    "                inp = (attr, encoded_feature, s_t, feature_dim)\n",
    "\n",
    "            g_t, updated_ocr = feature_embedding_model(*inp)\n",
    "            return g_t, updated_ocr\n",
    "\n",
    "    def combine_embeddings(self, *args):\n",
    "        feature_names = args[0]\n",
    "        feature_embeddings = args[1]\n",
    "\n",
    "        layer = \"_\".join(feature_names) + \"_multi_modal_combine_layer\"\n",
    "        return getattr(self, layer)(*feature_embeddings)\n",
    "\n",
    "    def calculate_logits(self, joint_embedding, **kwargs):\n",
    "        return self.classifier(joint_embedding)\n",
    "\n",
    "    def forward(self, sample_list):\n",
    "        sample_list.text = self.word_embedding(sample_list.text)\n",
    "        text_embedding_total = self.process_text_embedding(sample_list)\n",
    "\n",
    "        image_embedding_total, _ = self.process_feature_embedding(\n",
    "            \"image\", sample_list, text_embedding_total\n",
    "        )\n",
    "\n",
    "        if self.inter_model is not None:\n",
    "            image_embedding_total = self.inter_model(image_embedding_total)\n",
    "\n",
    "        joint_embedding = self.combine_embeddings(\n",
    "            [\"image\", \"text\"], [image_embedding_total, text_embedding_total]\n",
    "        )\n",
    "\n",
    "        model_output = {\"scores\": self.calculate_logits(joint_embedding)}\n",
    "\n",
    "        return model_output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
